一、ROC-AUC 和 PR-AUC
1、ROC曲线 Receiver Operating Characteristic Curve 受试者工作特征曲线
   横轴是假正率 False Positive Rate FPR， FPR = FP/(FP + TN) = 误判为真的负样本 / 所有负样本
   纵轴是真正率 True Positive Rate， 就是Recall， TPR=TP/(TP+FN) = 正确判为真的正样本 / 所有正样本
   绘制方法：在不同分类阈值下，计算（FPR，TPR）点，连成曲线
2、PR曲线（Precision-Recall Curve）
   横轴：召回率Recall Recall = TP/(TP + FN)
   纵轴：精确率（Precision）= TP/（TP + FP） = 正确判为真的正样本 / 所有被判为正的样本
   绘制方法：在不同分类阈值下，计算（Recall，Precision）点，连接成曲线
3、对比
（1） 关注重点
ROC-AUC：整体分类性能，兼顾正负类。
PR-AUC：主要关注正类（少数类）的性能
（2）受类别不平衡影响
ROC-AUC：相对不敏感
PR-AUC：非常敏感
（3）随机分类器基准
ROC-AUC:0.5 对角线
PR-AUC:正类比例，比如正类占10%，则基准就是0.1
（4）适用场景
ROC-AUC：类别相对平衡或者同等关注正负类，对模型进行初步评估和比较
PR-AUC：类别高度不平衡且更关注正类
（5）可视化解释
ROC-AUC：直观，有明确的随机线
PR-AUC：无固定随机线，需要计算基准

4、AUC的统计意义
所有AUC曲线的统计意义就是：随机选择一个正样本和一个负样本，模型将正样本预测为正的概率高于将负样本预测为正的概率。
用公式表示就是 AUC = P(模型给正样本的打分 > 模型给负样本的打分)
这个概率只依赖于模型对样本的排序能力，与正负样本的比例（类别分布）无关。
为什么说这种排序的统计意义使得ROC-AUC具有鲁棒性？
（1）对类别不敏感
样本从平衡变成不平衡，准确率会严重偏向多数类，失去参考价值。PR-AUC曲线会剧烈变化，因为其收到正样本影响的比例很大。但是ROC-AUC几乎不变，
因为它衡量的是模型对任一一对正负样本的排序能力，而不是绝对预测概率的准确性。
（2）对决策阈值选择不敏感
ROC曲线通过遍历所有的可能的分类阈值绘制，因此AUC综合了所有阈值下的表现。当实际应用的分类阈值因为业务需求变化时，只要模型的排序能力不变，AUC就不变。
（3）对样本绝对数量变化不敏感
只要测试集能代表真实分布，正负样本数量变化（比例不能改变）对AUC的影响很小，因为它基于成对比较的概率估计。

数学解释：AUC = （所有正样本后面的负样本数之和）/ (正样本数 * 负样本数）

5、如何将ROC-AUC的概率转换为 统计意义的排序
模型为每个样本输出一个“正类得分”（得分越高越可能是正类），给定阈值t，如果得分≥t，预测为正类
（1）绘制曲线
将所有样本按照得分从高到低排序。初始值阈值是+∞，TPR=0，FPR=0，就是左下角。
然后从得分最高的样本开始，逐个降低阈值。
每次固定一个阈值，本质是一次排序切片。每下降一次阈值发生了什么？
ROC的面积在数什么？ ROC-AUC曲线下面积就是在FPR增长过程中，TPR的累计高度。
当我们遇到一个负例时，TPR表示：当前阈值下，被排在这个负例前面的正例比例！！！
也就是 TPR = #{x+ : s(x+) > s(x-)}/正例
然后把所有负例加起来 AUC = ∑ 1/N * #{x+ : s(x+) > s(x-)}/正例 = 1/PN * ∑[S(x+) > s(x-)]
所以这就是排序

4、实践建议
（1）同时报告两个指标
（2）分析类别分布：先检查正负类比例，决定主要关注哪个指标
（3）结合业务需求：
    如果假阳性（明明是负例样本被判断成了阳性，也就是把错的判断成了对的）代价高，则关注PR-AUC和精度。比如患病筛查，就需要结合FPR的表现来结合AUC曲线，医疗诊断需要极低的FPR，可能选择0.01FPR对应的阈值，也就是大夫不能把没病的（负例样本）看成有病的（判断成了正例），也就是误诊多。
        如果FNR较高，就是漏诊多，把正例样本（患病）判断成了没病（negative）
    如果漏检代价高，则关注ROC-AUC和召回率
（4）使用 PR-AUC 可以进行阈值选择：PR曲线可以帮助找到精度和召回率的平衡点。
（5）警惕ROC-AUC的虚高：在高度不平衡的数据集中，即使ROC-AUC很高，实际业务可能效果不佳。
黄金法则：类别平衡先看ROC-AUC；类别不平衡且关注正类（少样本的）先看PR-AUC；重要应用：两个都看，结合具体阈值分析。

二、电商购物中的关联规则的置信度、支持度和Lift提升度
例子：电商平台使用关联规则挖掘提升交叉销售，当A--->B的置信度为85%，而支持度仅为0.1%,这说明：
（1）支持度Support0.1%，意味着A和B同时出现的交易占所有交易的0.1%，也就是100，000笔交易中，只有100比同时包含A和B
（2）置信度Confidence 85%，意味着购买A的顾客，有85%也购买了B，这是一个非常强的关联关系。

Lift提升度
Lift(A=>B)=Confidence(A=>B)/Support(B) = P(B|A)/P(B),等价表达式：Lift=Support(A∪B)/(Support(A)*Support(B))
Lift=1，表示A和B的购买是相互独立的没有关联性。
Lift<1,说明P(B|A)<P(B),说明在购买A后再购买B的概率比单独买B的概率还低了，说明购买A反而降低了购买B的欲望
提升度才是衡量关联规则价值的标准。

二、关联规则的序列模式挖掘算法及演变
1、序列挖掘与普通关联规则的区别
关联规则Apriori：只关注项集同时出现，购物篮分析。
序列模式：关注按时间顺序出现的模式，如用户购物行为分析
2、算法演进脉络
AprioriAll(1995)最早的序列挖掘算法--> GSP(1996)改进版，更高效 --> FreeSpan（2000）模式增长方法 --> PrefixSpan(2001)目前最主流高效的方法

3、ApiroriAll
将Apriori思想扩展到序列数据
算法步骤：（1）转换，将序列数据转换为大项集
（2）转换，将序列映射到大项集序列；
（3）排序，按照支持度排序大项集
（4）挖掘，类似Apriori，但是针对序列。
示例：
原始序列数据：
客户1: <(A,B), (C), (D,E)>  # 3次交易
客户2: <(A), (C,D), (B)>
AprioriAll会：
1. 找频繁项集：{A}, {B}, {C}, {D}, {AB}等
2. 映射为：客户1: <{AB}, {C}, {DE}>
3. 然后找频繁序列：如 <{A}, {C}>（先买A，后买C）
缺点：需要多次扫描数据库，产生大量候选序列，效率低。

4、GSP Generalized Sequential Patterns
核心思想：AprioriAll的优化版，引入时间约束和滑动窗口
关键创新：时间约束：最小时间间隔min-gap，最大时间间隔max-gap，窗口大小window size。剪枝策略，基于Apriori原理。
示例
数据：
Seq1: <(1,2), (3), (4,5)>   # 时间戳: t1, t2, t3
Seq2: <(1), (3,4), (2,5)>
约束：min-gap=0, max-gap=2, window=1
可能发现：
<1, 3>（1和3最多间隔2个时间单位）
<1, 2, 5>（按顺序出现）
优势：比AprioriAll快2-20倍，支持时间约束，更实用。缺点：仍然需要多次扫描数据库，候选序列数量仍然很大。

5、FreeSpan(Frequent pattern-projected Sequentail pattern mining)范式转变
核心思想：模式增长Pattern-Growth，不再产生候选序列
关键创新：（1）分治策略，将数据库按照频繁项划分，（2）投影数据库，为每个频繁项集创建子数据库；（3）递归挖掘，在每个投影库中递归挖掘
算法思想：原始数据库 D
     频繁项列表：f_list = [f1, f2, ..., fn]
     对于每个频繁项 fi：
         1. 构建 fi-投影数据库 D|fi（只包含fi及之后的序列部分）
         2. 在 D|fi 中递归挖掘以fi为前缀的序列模式
例子：数据库：
   S1: <a, abc, ac, d, cf>
   S2: <ad, c, bc, ae>
   S3: <ef, ab, df, c, b>
   S4: <e, g, af, c, b, c>

   频繁项（min_sup=2）：a,b,c,d,e,f

   对于频繁项'a'：
   1. 构建'a'的投影数据库：
      S1投影: <abc, ac, d, cf>  # 从a之后开始
      S2投影: <d, c, bc, ae>
      S3投影: <>  # 没有a
      S4投影: <f, c, b, c>

   2. 在投影库中递归挖掘以'a'开头的序列模式
优势：避免候选生成，减少数据库扫描次数，更高效；缺点：投影数据库可能很大，内存消耗可能高。

6、PrefixSpan(Prefix-projected Sequential pattern mining)当前主流，工业标准
核心思想：FreeSpan的改进版，只投影前缀，更节省空间
关键改进：FreeSpan是按照频繁项投影，PrefixSpan：按照前缀投影，更精确。
示例：
原始数据库：
S1: <a, abc, ac, d, cf>
S2: <ad, c, bc, ae>
S3: <ef, ab, df, c, b>
S4: <e, g, af, c, b, c>
挖掘过程：
1. 找频繁1-序列：a,b,c,d,e,f
2. 以'a'为前缀：
   - 构建'a'投影数据库
   - 在投影库中找频繁项：b,c,d,f
   - 输出：<a,b>, <a,c>, <a,d>, <a,f>
   - 对<a,b>：递归构建<a,b>投影库，继续挖掘
优势：目前最高效的序列挖掘算法，不需要候选生成，投影数据库更小，支持各种约束。

三、可视化降维技术之 t-SNE
t-SNE（t-Distributed Stochastic Neighbor Embedding）是一种非线性降维技术，专门为高维数据可视化设计。
核心目标：将高维空间中的数据点映射到低维（通常是2D/3D）空间，保持数据点之间的相对相似性，使得在原始高维空间中相似的点在低维空间中靠近，不相似的点远离。
t-sne图簇内紧密，簇间有空隙，可识别异常用户（离群点）
PCA与t-SNE:PCA是一种线性变换，旨在最大化数据的方差，保留的是全局结构，t-SNE是一种非线性方法，更擅长保留数据的局部结构。PCA的计算是确定性的，对于同一份数据，
每次运行结果都相同；而t-SNE包含随机过程，每次运行结果可能会有细微差异，除非固定随机种子。由于t-SNE计算复杂度较高，一个常见的实践方式是先用PCA将数据降维
到中等维度，例如50维，然后再利用t-SNE进行二维或者三维的可视化。

三、可视化技术之 平行坐标技术
平行坐标技术是一种专门用于处理高维数据可视化的方法，它通过将N维空间映射到2维平面上来展示数据，能有效处理4维以上的数据。其原理是将每个维度绘制
成一条垂直的坐标轴，这些轴平行排列，数据点则通过连接各个轴上的对应值形成折线。这种技术的优点在于：可以同时展示多个维度的数据关系，能够直观显示数据
各维度之间的相关性，适合进行多维度数据的模式识别和趋势分析。但是，当数据量很大时，大量的线条就会相互交叠，造成视觉混乱降低可读性。

四、局部异常因子 LOF（Local Outlier Factor）：基于局部密度的异常检测
核心思想：异常点是其局部领域内的密度显著低于其邻居的点。
与传统基于全局距离的方法不同，LOF考虑了数据的局部密度变化，能检测局部异常。

五、关于用户行为和商品的问题
1、为了提升推荐系统的覆盖率，需要识别长尾商品，NDCG 和 基尼系数这两个评估指标最能平衡准确性和多样性
2、在电商用户行为分析中，发现某商品购买率随着用户浏览时长增长而显著上升，但是存在个别异常点，为确定变量间关联强度并降低噪声干扰，
最应使用的数据挖掘指标是“斯皮尔曼等级相关系数”。这种情况需要确定变量间关联强度并降低噪声干扰，斯皮尔曼等级相关系数基于数据的秩计算，
对异常点不敏感，能有效测量变量间的单调关系并降低噪声影响。有异常值选择斯皮尔曼相关系数，无异常值选择皮尔逊相关系数。严格的参数校验选择皮尔逊相关系数，非参数校验选择斯皮尔曼相关系数
其他相关系数，在该问题中解决能力不足（1）皮尔逊相关系数对异常点敏感，会增加噪声。
（2）欧几里德距离是距离度量，不直接用于关联强度分析。（3）Jaccard相似系数，适用于二元数据而非连续型变量。
3、一个金融风控团队使用了一个复杂的XGBoost模型来评估贷款申请的风险。现在需要向审计部门解释，为什么张三的贷款申请拒绝了。
为了给这个单一的具体的预测提供量化的归因解释，哪个技术或者工具更实用？
计算并展示模型全局的特征重要性排序、绘制关键特征的依赖图PDP、计算并分析对张三这个样本的SHAP值。在整个测试集上进行置换 重要性分析。
SHAP值分析是最实用、最有效的单一预测解释工具。一致性：SHAP值有坚实的博弈论基础；可加性：所有特征的SHAP值之和=预测值-基线值；
（3）直观性：正值为风险增加，负值为风险降低（4）可量化：精确到每个特征对张三的影响数值。
计算方法：公平分配贡献值，将预测结果公平地分配给每个特征，假设模型预测张三违约的概率是0.8，而平均违约概率基线是0.3，那么需要解释为什么张三比平均
水平高了0.5.SHAP值就是把整个额外风险0.5公平地分配给每个特征。
4、在对一个大规模、高纬度的用户行为数据进行密集异常点检测时，发现异常用户的行为模式非常多样化，且不构成一个或多个密集的异常簇，而是表现为原理任何正常行为
模式的孤立点。在这种场景下，DBSCAN最适合用来识别这些异常用户。因为它能通过密度定义识别低密度区域的孤立点作为噪声点，而不假设数据点必须形成簇。
5、在构建一个预测模型时，数据集中包含一个具有很高基数（high cardinality）的分类特征，例如“用户ID”，它有百万个唯一值。使用标签编码是最不推荐的。
因为标签编码将每个类别映射为一个唯一的整数，但是线性模型会错误地解释这些整数之间的序列，而用户ID是名义变量Norminal variable，没有内在顺序关系，
这会导致模型预测偏差或者性能下降。
可以使用目标编码，在防止泄露处理后，也要需要注意过拟合。特征哈希，适合线性模型，能够有效降维。频率编码作为数值特征也可以接受。
6、某电商平台对高维稀疏的用户行为数据，如用户对商品的点击、收藏、购买等0-1特征，进行用户分群，谱聚类，基于图的相似性聚类算法更适合该场景。
7、某电商平台希望对用户进行分群，以便进行精细化运营。业务方无法预先确定具体的用户群数量，并希望能够看到用户群体之间可能存在的层级关系，
例如，高价值用户群体下可分为高消费高频率用户和高消费低频率用户。在这种场景下，相比Kmeans算法，凝聚型层次聚类Agglomerative Hierarchical Clustering
更适合，因为它不需要预设簇的数量，并且其输出的树状图Dendrogram可以直观地展示簇间的层级结构。
8、你正在使用线性回归模型预测电商网站的销售额。你注意到，有一个预测变量的系数是负的，但你预期它应该是正的。这可能是什么原因？
预测变量和其他预测变量存在多重共线性

六、SMOTE 解决类别不平衡的智能过采样技术
核心问题：类别不平衡的困境。
SMOTE核心思想：在特征空间内智能创造新样本，Synthentic Minority Over-Sampling Technique不是简单的复制，而是在少数类样本之间创造新样本。


七、目标编码 Target Encoding
目标编码是一种利用目标变量信息对分类变量进行编码的强大技术，特别适合监督学习任务。核心思想是用目标统计量代表类别，用每个类别的目标变量统计量
（如均值、中位数、出现概率等）来编码来类别。其中，为了避免小样本类别的过拟合，平滑目标编码可以防止过拟合。


八、ISODATA 智能动态聚类算法
核心思想：K-Means的智能进化版
ISODATA（Iterative Self-Organizing Data Analysis Technique）是K-means的自适应、动态版本，可以自动确定最佳聚类数。
动态调整K值，自动增加或者减少聚类数量；智能合并/分裂：根据簇的形态动态调整。考虑簇的分布特性：大小紧密度、分离度。
ISODATA算法在聚类过程中引入了对类别的评价准则，可以根据这些准则自动对类别合并或者分裂。

九、几个树模型的核心优化点
1、XGBoost的核心优化点：
 （1）在目标函数中加入L1和L2正则化项，以控制模型复杂度防止过拟合，这在高维稀疏数据中尤为重要。
 （2）XGBoost的分裂点查找使用直方图近似，但并未默认将所有连续特征分箱成固定数量的bins（箱子）
 （3）XGBoost默认使用深度优先策略 depth-wise，而不是level-wise。
2、LightGBM：引入基于梯度的单边采样Goss，优先关注梯度大的样本以加速训练。
3、

