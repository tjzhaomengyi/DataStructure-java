一、ROC-AUC 和 PR-AUC
1、ROC曲线 Receiver Operating Characteristic Curve 受试者工作特征曲线
   横轴是假正率 False Positive Rate FPR， FPR = FP/(FP + TN) = 误判为真的负样本 / 所有负样本
   纵轴是真正率 True Positive Rate， 就是Recall， TPR=TP/(TP+FN) = 正确判为真的正样本 / 所有正样本
   绘制方法：在不同分类阈值下，计算（FPR，TPR）点，连成曲线
2、PR曲线（Precision-Recall Curve）
   横轴：召回率Recall Recall = TP/(TP + FN)
   纵轴：精确率（Precision）= TP/（TP + FP） = 正确判为真的正样本 / 所有被判为正的样本
   绘制方法：在不同分类阈值下，计算（Recall，Precision）点，连接成曲线
3、对比
（1） 关注重点
ROC-AUC：整体分类性能，兼顾正负类。
PR-AUC：主要关注正类（少数类）的性能
（2）受类别不平衡影响
ROC-AUC：相对不敏感
PR-AUC：非常敏感
（3）随机分类器基准
ROC-AUC:0.5 对角线
PR-AUC:正类比例，比如正类占10%，则基准就是0.1
（4）适用场景
ROC-AUC：类别相对平衡或者同等关注正负类，对模型进行初步评估和比较
PR-AUC：类别高度不平衡且更关注正类
（5）可视化解释
ROC-AUC：直观，有明确的随机线
PR-AUC：无固定随机线，需要计算基准

4、实践建议
（1）同时报告两个指标
（2）分析类别分布：先检查正负类比例，决定主要关注哪个指标
（3）结合业务需求：
    如果假阳性（明明是负例样本被判断成了阳性，也就是把错的判断成了对的）代价高，则关注PR-AUC和精度。比如患病筛查，就需要结合FPR的表现来结合AUC曲线，医疗诊断需要极低的FPR，可能选择0.01FPR对应的阈值，也就是大夫不能把没病的（负例样本）看成有病的（判断成了正例），也就是误诊多。
        如果FNR较高，就是漏诊多，把正例样本（患病）判断成了没病（negative）
    如果漏检代价高，则关注ROC-AUC和召回率
（4）使用 PR-AUC 可以进行阈值选择：PR曲线可以帮助找到精度和召回率的平衡点。
（5）警惕ROC-AUC的虚高：在高度不平衡的数据集中，即使ROC-AUC很高，实际业务可能效果不佳。
黄金法则：类别平衡先看ROC-AUC；类别不平衡且关注正类（少样本的）先看PR-AUC；重要应用：两个都看，结合具体阈值分析。

二、电商购物中的关联规则的置信度、支持度和Lift提升度
例子：电商平台使用关联规则挖掘提升交叉销售，当A--->B的置信度为85%，而支持度仅为0.1%,这说明：
（1）支持度Support0.1%，意味着A和B同时出现的交易占所有交易的0.1%，也就是100，000笔交易中，只有100比同时包含A和B
（2）置信度Confidence 85%，意味着购买A的顾客，有85%也购买了B，这是一个非常强的关联关系。

Lift提升度
Lift(A=>B)=Confidence(A=>B)/Support(B) = P(B|A)/P(B),等价表达式：Lift=Support(A∪B)/(Support(A)*Support(B))
Lift=1，表示A和B的购买是相互独立的没有关联性。
Lift<1,说明P(B|A)<P(B),说明在购买A后再购买B的概率比单独买B的概率还低了，说明购买A反而降低了购买B的欲望
提升度才是衡量关联规则价值的标准。

三、可视化降维技术之 t-SNE
t-SNE（t-Distributed Stochastic Neighbor Embedding）是一种非线性降维技术，专门为高维数据可视化设计。
核心目标：将高维空间中的数据点映射到低维（通常是2D/3D）空间，保持数据点之间的相对相似性，使得在原始高维空间中相似的点在低维空间中靠近，不相似的点远离。
t-sne图簇内紧密，簇间有空隙，可识别异常用户（离群点）
PCA与t-SNE:PCA是一种线性变换，旨在最大化数据的方差，保留的是全局结构，t-SNE是一种非线性方法，更擅长保留数据的局部结构。PCA的计算是确定性的，对于同一份数据，
每次运行结果都相同；而t-SNE包含随机过程，每次运行结果可能会有细微差异，除非固定随机种子。由于t-SNE计算复杂度较高，一个常见的实践方式是先用PCA将数据降维
到中等维度，例如50维，然后再利用t-SNE进行二维或者三维的可视化。

三、可视化技术之 平行坐标技术
平行坐标技术是一种专门用于处理高维数据可视化的方法，它通过将N维空间映射到2维平面上来展示数据，能有效处理4维以上的数据。其原理是将每个维度绘制
成一条垂直的坐标轴，这些轴平行排列，数据点则通过连接各个轴上的对应值形成折线。这种技术的优点在于：可以同时展示多个维度的数据关系，能够直观显示数据
各维度之间的相关性，适合进行多维度数据的模式识别和趋势分析。但是，当数据量很大时，大量的线条就会相互交叠，造成视觉混乱降低可读性。

四、局部异常因子 LOF（Local Outlier Factor）：基于局部密度的异常检测
核心思想：异常点是其局部领域内的密度显著低于其邻居的点。
与传统基于全局距离的方法不同，LOF考虑了数据的局部密度变化，能检测局部异常。

五、关于用户行为和商品的问题
1、为了提升推荐系统的覆盖率，需要识别长尾商品，NDCG 和 基尼系数这两个评估指标最能平衡准确性和多样性
2、在电商用户行为分析中，发现某商品购买率随着用户浏览时长增长而显著上升，但是存在个别异常点，为确定变量间关联强度并降低噪声干扰，
最应使用的数据挖掘指标是“斯皮尔曼等级相关系数”。这种情况需要确定变量间关联强度并降低噪声干扰，斯皮尔曼等级相关系数基于数据的秩计算，
对异常点不敏感，能有效测量变量间的单调关系并降低噪声影响。有异常值选择斯皮尔曼相关系数，无异常值选择皮尔逊相关系数。严格的参数校验选择皮尔逊相关系数，非参数校验选择斯皮尔曼相关系数
其他相关系数，在该问题中解决能力不足（1）皮尔逊相关系数对异常点敏感，会增加噪声。
（2）欧几里德距离是距离度量，不直接用于关联强度分析。（3）Jaccard相似系数，适用于二元数据而非连续型变量。
3、一个金融风控团队使用了一个复杂的XGBoost模型来评估贷款申请的风险。现在需要向审计部门解释，为什么张三的贷款申请拒绝了。
为了给这个单一的具体的预测提供量化的归因解释，哪个技术或者工具更实用？
计算并展示模型全局的特征重要性排序、绘制关键特征的依赖图PDP、计算并分析对张三这个样本的SHAP值。在整个测试集上进行置换 重要性分析。
SHAP值分析是最实用、最有效的单一预测解释工具。一致性：SHAP值有坚实的博弈论基础；可加性：所有特征的SHAP值之和=预测值-基线值；
（3）直观性：正值为风险增加，负值为风险降低（4）可量化：精确到每个特征对张三的影响数值。
计算方法：公平分配贡献值，将预测结果公平地分配给每个特征，假设模型预测张三违约的概率是0.8，而平均违约概率基线是0.3，那么需要解释为什么张三比平均
水平高了0.5.SHAP值就是把整个额外风险0.5公平地分配给每个特征。
4、在对一个大规模、高纬度的用户行为数据进行密集异常点检测时，发现异常用户的行为模式非常多样化，且不构成一个或多个密集的异常簇，而是表现为原理任何正常行为
模式的孤立点。在这种场景下，DBSCAN最适合用来识别这些异常用户。因为它能通过密度定义识别低密度区域的孤立点作为噪声点，而不假设数据点必须形成簇。
5、在构建一个预测模型时，数据集中包含一个具有很高基数（high cardinality）的分类特征，例如“用户ID”，它有百万个唯一值。使用标签编码是最不推荐的。
因为标签编码将每个类别映射为一个唯一的整数，但是线性模型会错误地解释这些整数之间的序列，而用户ID是名义变量Norminal variable，没有内在顺序关系，
这会导致模型预测偏差或者性能下降。
可以使用目标编码，在防止泄露处理后，也要需要注意过拟合。特征哈希，适合线性模型，能够有效降维。频率编码作为数值特征也可以接受。
6、某电商平台对高维稀疏的用户行为数据，如用户对商品的点击、收藏、购买等0-1特征，进行用户分群，谱聚类，基于图的相似性聚类算法更适合该场景。
7、某电商平台希望对用户进行分群，以便进行精细化运营。业务方无法预先确定具体的用户群数量，并希望能够看到用户群体之间可能存在的层级关系，
例如，高价值用户群体下可分为高消费高频率用户和高消费低频率用户。在这种场景下，相比Kmeans算法，凝聚型层次聚类Agglomerative Hierarchical Clustering
更适合，因为它不需要预设簇的数量，并且其输出的树状图Dendrogram可以直观地展示簇间的层级结构。


六、SMOTE 解决类别不平衡的智能过采样技术
核心问题：类别不平衡的困境。
SMOTE核心思想：在特征空间内智能创造新样本，Synthentic Minority Over-Sampling Technique不是简单的复制，而是在少数类样本之间创造新样本。


七、目标编码 Target Encoding
目标编码是一种利用目标变量信息对分类变量进行编码的强大技术，特别适合监督学习任务。核心思想是用目标统计量代表类别，用每个类别的目标变量统计量
（如均值、中位数、出现概率等）来编码来类别。其中，为了避免小样本类别的过拟合，平滑目标编码可以防止过拟合。


八、ISODATA 智能动态聚类算法
核心思想：K-Means的智能进化版
ISODATA（Iterative Self-Organizing Data Analysis Technique）是K-means的自适应、动态版本，可以自动确定最佳聚类数。
动态调整K值，自动增加或者减少聚类数量；智能合并/分裂：根据簇的形态动态调整。考虑簇的分布特性：大小紧密度、分离度。
ISODATA算法在聚类过程中引入了对类别的评价准则，可以根据这些准则自动对类别合并或者分裂。

九、几个树模型的核心优化点
1、XGBoost的核心优化点：
 （1）在目标函数中加入L1和L2正则化项，以控制模型复杂度防止过拟合，这在高维稀疏数据中尤为重要。
 （2）XGBoost的分裂点查找使用直方图近似，但并未默认将所有连续特征分箱成固定数量的bins（箱子）
 （3）XGBoost默认使用深度优先策略 depth-wise，而不是level-wise。
2、LightGBM：引入基于梯度的单边采样Goss，优先关注梯度大的样本以加速训练。
3、

