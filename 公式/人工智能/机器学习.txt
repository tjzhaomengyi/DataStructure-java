一、图像相关
1、图像搜索引擎开发
    用户可以通过上传一张商品图片来搜索相似的商品，在这个任务中，图像特征分析对于提高搜索准确性非常关键。需要使用直方图均衡化来解决这个任务。
直方图均衡化通过重新分布图像的像素值，使得像素值更均匀分布。在图像搜索引擎中，可以使用直方图均衡化来调整商品图像的对比度，提高搜索引擎对不同商品
之间的细节特征的捕捉能力。

二、交叉验证
1、有一个二分类数据集，共 10 个样本，其中正例 5 个、负例 5 个。学习器的训练策略为：在训练集上预测“出现次数最多的类别”（训练集若出现并列多数则固定预测负类；本题 2 折设置下不会出现并列）。评价规则如下：
2 折交叉验证（非分层、等概率随机二分）：将数据随机等分为两折（各 5 个），用第 1 折训练在第 2 折测试，再用第 2 折训练在第 1 折测试，取两次测试错误率的平均；然后对“所有可能的等概率随机二分划分”取期望，得到“2 折交叉验证的期望错误率”。
留一法（LOOCV）：每次划去 1 个样本作测试，其余 9 个作训练，10 次测试错误率取平均。
问：2 折交叉验证的期望错误率与留一法的错误率分别为多少？
答案 ：选择一个固定的样本x，这个样本会出测试集，但是肯定不会出现在训练集，1折就是1个样本做测试，剩下的9个全部去做训练。
1、留一法，错误率肯定是1。剩下9个少了没有训练的那个样本，肯定决策是这个固定样本分类的反类，所以判定的时候必然和固定样本不是同一个类。
2、五折法，就是用5个训练，5个测试。假设5折中有k个正例，5-k个负例，每种情况的概率P(k)=(C_5,k * C_5,5-k) / C_10,5
对于每个k，计算两折平均错误率,A-->B错误率，也就是使用训练集A得到的模型来预测集合B中结果错误的概率，比如训练集A中有4正1负，那么模型肯定是预测为正，
那么此时测试集B中只有1正4负，所以只能预测1个对的，剩下四个都是错的，错误率4/5
k           A中正例        A->B错误率
5           5P 0N           1
4           4P 1N           4/5
3           3P 2N           3/5
2           2P 3N           3/5
1           1P 4N           4/5
0           0P 5N           1
所以最终期望就是E=1/252 * 1 + 25/252 * 4/5 + 100/252 * 3/5 + 100/252 * 3/5 + 25/252 * 4/5 + 1/252 * 1 = 9/14

三、回归模型
1、使用回归模型后，发现模型的部分特征线性相关性较高，这样会导致如下情况：
（1）特征之间的共线性会导致回归系数（权重）估计不稳定，容易使得预测结果的方差变大。
（2）共线性的存在不利于模型的可解释性分析
（3）多个特征之间具有强线性关系不利于准确定义每个特征对预测的独立贡献。

四、假设检验
P值用于衡量统计假设检验中观察到的样本数据与原假设之间的差异程度的一种指标。
P值告诉我们，如果H0是真的，那么得到我们观察到的这么极端或者更极端的样本的概率是多少？
如果P值很小，< 0.05:意味着在原假设成立的情况下，观察到当前数据是非常不可能的事件。既然这么不可能的事情发生了，我们就有理由怀疑原假设不成立，拒绝H0
如果P值很大，> 0.1：意味着在原假设成立的情况下，观察到当前数据很可能发生的。因此，没有理由拒绝H0，不拒绝H0

五、迁移学习
领域自适应是迁移学习的一种形式，旨在解决源领域和目标领域之间数据分布的差异。
挑战1：在领域自适应中，目标领域的数据分布通常会发生变化，这是一种挑战，因为源领域上学习到的模型可能无法直接适应目标领域的数据。
挑战2：另一个挑战是缺乏目标领域的标签信息，在许多情况下，目标领域的标签信息很少或者根本没有，这使得直接在目标领域上进行监督学习变得非常困难，需要使用无监督或者半监督的方法来进行迁移学习。
挑战3：此外，领域自适应中还需要解决领域间特征不一致的问题。不同领域之间可能存在特征表示的差异，这使得在目标领域上直接使用源领域的特征表示可能效果不佳。因此需要通过特征选择、特征映射或者领域间进行对抗训练的那个发来解决领域间特征不一致的问题。

六、Cohen's Kappa系数
当类别分布极度不平衡时，一个总是预测多数的模型就能得到99%的准确，这显然是没有意义的。Kappa系数的思路，比较实际观测到的一致性（准确率）与纯粹
由随机猜测带来的预期一致性，然后看实际表现比随机猜测好多少。
k=(po-pe)/(1-pe)
po=观测到的一致性比例=准确率 pe=随机预测的一致性比例

七、深度神经网络
在深度神经网络训练中，批量归一化Batch Normalization层被广泛应用于加速收敛和提升模型性能。除了主要目标：解决内部协变量偏移Internal Covariate SHift
问题外，还有一个“副作用”--它通过在每个mini-batch上计算均值和方差引入了轻微的噪声，这种噪声起到了正则化的作用，有助于减少模型对dropout的依赖。

八、树模型
（一）Histogram 直方图技术
XGBoost中的直方图Historgram优化技术。这实际上是LightGBM的核心创新，但是XGBoost在后续版本中加入了类似优化。
1、直方图优化的核心动机
核心动机就是传统树分裂方法的瓶颈，在普通的决策树，包括早期的XGBoost中，每次寻找最优分裂点需要：
（1）对每个特征的所有唯一值进行排序
（2）遍历所有可能的分裂点计算增益
（3）对于连续特征，如果特征值很多，如浮点数，计算成本很高
直方图的基本思想，将连续特征的值离散化到规定的箱子中bin，然后在这些箱子上寻找最优分裂点，而不是在原始特征值上。
步骤1：特征离散化（分箱）
步骤2：基于直方图寻找最优分裂
步骤3：分裂执行
（二）关于XGBoost和LightGBM模型的说法：
1、XGBoost的直方图并不是针对某个特定的特征，而是所有特征都共享一个直方图
XGBoost的直方图策略：（1）按层构建，在每个树节点分裂时，为该节点需要处理的特征动态构建直方图（2）共享内存，直方图数据结构在不同特征间复用，同一时间只处理一个特征的直方图
（3）建后即弃：节点分裂完成后，直方图通常被丢弃或者覆盖
LightGBM的直方图策略：（1）按照特征预先构建：在训练开始时，为每个特征预先构建完整的直方图（2）独立存储：每个特征有自己独立的直方图存储，全称保持不变
（3）增量更新：通过直方图技术，从父节点直方图快速计算子节点直方图
两者设计哲学度对比：XGBoost的直方图设计，兼容性灵活性好，节省内存，但是增加计算，特征并行，每个线程处理一个特征，通用性好支持各种场景
LightGBM：极致的速度和内存效率，预先分配内存，加速计算，数据并行 + 特征并行，大数据场景下的极致性能。

2、Block结构是XGBoost速度慢的一个重要因素，它通过索引来获取梯度，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得缓存命中率低，
从而影响算法效率。而LightGBM是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。
Block是XGBoost中按照特征排序的数据存储单元。它不是存储原始的特征值，而是存储排序后的索引和对应的梯度值。
Block结构引入了两层间接访问：（1）需要找到节点样本在排序数组中的位置 （2）通过位置索引获取梯度值。这就导致了缓存不友好：梯度值不是连续的，额外索引查找：需要维度样本ID到排序位置的映射。
Block结构和直方图技术的关系，（1）Block是基础数据结构，XGBoost都用BLock存储排序后的梯度。（2）直方图是计算优化，在Block基础上，进一步聚合梯度到bin中
LightGBM使用特征并行直方图，完全避免了Block结构。
LightGBM的数据流：原始数据 --> 一次性分箱 --> 直方图数组 --> 训练时直接访问
XGBoost的数据流： 原始数据 --> Block（排序索引 + 梯度） --> 训练时动态构建直方图 --> 需要索引查找 + 梯度获取

3、XGBoost使用level-wise 的分裂策略，LightGBM 采用 leaf-wise 的分裂策略
4、LightGBM的直方图算法相比XGBoost能够减低worker间的通信成本，加快算法的运行。在大数据量下，LightGBM中先对数据做水平切分，每个worker上的数据
先建立起局部的直方图，然后合并成全局的直方图。采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一个子节点的样本索引，
因此大大降低了worker间的通信成本。因为只用通信这些样本量少的节点。
XGBoost中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点
的样本索引，因此效率很低，每个worker间的通信量非常大。

5、在特征分割点查找时，如果将连续特征离散得到256个bin，LightGBM的直方图算法比XGBoost的特征预排序算法，节省7/8的空间
256是2^8,因此256个bin需要8为整形就可以保存某个样本被映射到哪个bin。而XGBoost的预排序算法需要32为整形来存储索引，32位浮点型来存储连续特征值，
一共需要64位。

6、XGBoost对异常值敏感。


（三）被错分的样本比例分配
AdaBoost算法错样本权重更新比例相同，
在Gradient Boosting 包括 XGBoost、LightGBM、CatBoost中，所有被错分的样本的权重更新比例是不同的。

（四） CART、ID3 和 C4.5树模型 以及 熵增益 和 熵增益比
ID3以信息增益大为分裂标准，C4.5以信息增益比大为分裂标准，CART以基尼增益大，分裂后基尼指数小为标准。
信息增益和信息增益比都可以用来评估特征重要性，其中信息增益衡量信息熵的减小，而信息增益比衡量的是信息熵的增加。
信息增益衡量信息熵的减少，值越大表示特征越重要。信息增益比是信息增益的归一化形式，信息增益除以分裂信息的熵，用于减少对多值特征的偏好，但是它并不衡量
信息熵的增加。相反，它是对信息增益的一种调整。

CART算法在构造决策树的时候，使用 基尼指数作为分裂的标准时，基尼指数越小说明分裂后更纯，分裂过程中基尼增益就越大。
基尼指数衡量数据的不纯度，基尼增益越高，说明特征分区能力强，基尼增益低，说明该特征分区能力若，基尼增益越高，特征越好。只要提到基尼系数低或者高，说明特征分区能力强的都是错误的

CART算法在分类任务中，选择基尼指数增益最大的特征进行分裂。
ID3和C4.5都不能处理回归任务，C4.5算法具备处理缺失值的能力，但ID3算法只能处理连续变量。ID3不能处理连续特征和缺失值。C4.5可以处理缺失值和连续特征。

（五）随机森林
1、随机森林支持并行训练，各个决策树独立生成，能有效利用多核CPU，通过BootStrap采样和特征随机选择减少计算开销，适合大数据集。
2、随机森林本身不需要先降维，它内置了特征随机选择机制，每棵树分裂时值考虑特征子集，能够自动处理高维特征，即使维度高于样本数，这是随机森林的优势。
3、能够评估各个特征在分类问题上的重要性。随机森林可以基于基尼不纯度减少或者基于OOB误差增加的方法为来评估特征的重要性，这是其常用功能之一。
4、随机森林【从来不过拟合，这特么】通过特征的随机选择（列采样）和样本的随机选择（行采样）来保证多样性，这种随机性能够有效防止过拟合。即使不进行剪枝，随机森林也通常表现出很好的泛化能力。
（六）Bagging 和 Boosting
Bagging即Bootstrap Aggregating 是一种集成学习方法，Bagging通过对训练集进行自助采样Bootstrap来产生不同的训练子集，每个子集训练一个基学习器。
由于采样的随机性，使得不同子集之间存在差异，从而使得基学习器之间的相关性较低，这有助于提高集成的泛化性能。
Bagging采用投票法来决定最终结果，对于回归任务使用简单平均法得到最终结果。
Bagging方法通过并行训练多个独立基模型，使用自助采样，并平均或者投票结果，可以有效降低模型的方差。因为Bagging可以降低方差，所以Bagging方法可以用来解决过拟合问题。
Boosting通过串行训练基模型，后续模型纠正前续模型的错误，主要降低偏差而不是方差。



七、高斯马尔可夫随机场GMRF
在GMRF中，每个变量对应图中的一个节点，精度矩阵Θ的非零非对角元素对应图中的边。
在真实应用中，如基因调控网络、脑功能连接等，我们观测到的高维数据，想发现变量间的直接交互关系，而不是间接关系：协方差矩阵的逆（精度矩阵）中的非零
元素指示直接作用；稀疏性意味着大多数变量之间没有直接作用，只有少数关键连接。

八、神经网络
1、训练了一个多类别神经网络用于实时决策，系统需要依赖模型输出的概率进行代价敏感的阈值设定，希望概率可以校准，同时又不希望改变现有的类别边界
argmax不变。在离线后处理阶段，最合适的概率校准方法是Temperature Scaling 对softmax logits进行温度缩放

九、逻辑回归
1、逻辑回归的多重共线性问题，就是两个或多个自变量之间存在高度线性相关的情况。模型难以区分每个特征的独立贡献，导致回归系数的估计值方差变大，稳定性降低。
2、检测多重共线性的方法
法1：方差膨胀因子
法2：相关系数矩阵
法3：条件数
3、解决方法
删除冗余变量、PCA、L1和L2正则(Lasso、Ridge和Elastic Net)、增大样本量、特征工程

十、概率校准 Probability Calibration
1、为什么要概率校准
许多机器学习模型，尤其是神经网络、Boosting模型输出的概率不可靠
过度自信：预测概率0.9，但实际正确率只有70%，自信不足：预测概率60%，但是实际正确率90%，影响决策：居于不可靠概率做代价敏感决策会出问题。
典型场景：医疗诊断、金融风控、自动驾驶，都需要概率能够真实反应“事件发生的概率”。
2、校准的核心思想
如果模型说我有90%的把握，那么在实际中九应该有90%的情况下它是对的。
3、主要校准方法：
第一类：分箱法
（1）直方图分箱
简单直观非参数；阶梯状输出，需要足够数据填满每个桶；适合快速基线方法，小到中等数据集
（2）等宽分箱
更光滑的映射，在单调性约束下理论上最优，可能过拟合，需要较多校准数据，适合中到大数据集，单调关系明显时。
第二类方法：参数化方法
（1）Platt Scaling
参数少，2个，不容易过拟合；需要假设线性关系，对SVM等输出适合，对神经网络可能不够，适用于SVM、提升树等输出的校准
（2）Temperature Scaling 温度缩放
保持类别排序不变argmax不变，单个参数简单高效，专门为神经网络设计，只调整整体置信度，不处理类别之间的差异。适用于神经网络的标准校准方法，多分类问题。
（3）向量/矩阵缩放
更灵活，可以处理更复杂的校准要求，参数多，容易过拟合。当温度缩放不够用时，有大量校准数据时，可以使用。
第三类：贝叶斯与集成方法
（1）Bayesian Binning into Quantiles
提供不确定性估计，计算复杂，需要不确定性量化的场景。
（2）Ensemble of Calibrators
非常灵活，可以捕捉复杂模式，复杂，可能过拟合，计算成本高，有大量校准数据，且关系复杂度高。
4、选择决策流程
你的模型是什么？
├─ 神经网络 → Temperature Scaling（首选）
├─ SVM/逻辑回归 → Platt Scaling
├─ 随机森林/提升树 → 先看是否已校准（通常不错）
└─ 其他 → 试试直方图分箱或Isotonic

有多少校准数据？
├─ 很少（<100）→ 直方图分箱（桶数少）
├─ 中等（100-1000）→ Platt或Isotonic
└─ 很多（>1000）→ 可以尝试更复杂的方法

是否需要保持排序？
├─ 是（如分类阈值固定）→ Temperature Scaling或向量缩放
└─ 否 → 任何方法都可以

需要多快？
├─ 实时推理 → Temperature Scaling、Platt Scaling
└─ 离线批处理 → 任何方法

十一、线性分类的经典历史算法
1. 有违逆法（Ho-Kashyap Algorithm）
核心思想：
解决线性分类问题，但不要求线性可分——允许一些样本被错分（有“违逆”）。
线性分类器 g(x) = w^Tx + b
对于正类：w^Txi + b ≥ 1 - ξi
对于负类：w^Txi + b ≤ -1 + ξi
ξi是松弛变量，允许违反间隔约束。
算法特点：
处理线性不可分：比感知器更实用
迭代求解：类似感知器但更复杂
历史意义：是SVM（软间隔）的前身之一

2、感知器算法（Perceptron Algorithm）
核心思想：
最著名的在线学习算法之一，用于寻找线性可分数据的分离超平面，非线性的处理不了。
经典算法为：
初始化 w = 0, b = 0
重复直到所有样本正确分类：
    对每个样本 (x_i, y_i) ∈ {+1, -1}:
        如果 y_i(w·x_i + b) ≤ 0:  # 分类错误
            w = w + η y_i x_i      # 更新权重
            b = b + η y_i          # 更新偏置
关键定理：感知器收敛定理
如果数据线性可分，感知器保证在有限步内收敛
优势：简单、在线学习、理论保证（可分时）
局限：仅适用于线性可分数据，否则永不收敛

3. 基于二次准则的H-K算法
这是有违逆法的一种变体，具体指Ho-Kashyap算法的二次规划版本。
数学优化问题：min1/2||w||²+C∑ξi^2，约束yi(w^T+b)≥1-ξi
与标准有违逆法的区别，包含权重正则项；惩罚项，松弛变量的平方而非线性
几何解释：同时最小化权重范数和分类误差
这是SVM的直接前身。

4、势函数法，
核心思想：用物理类比解决分类问题，将样本看作电荷，决策边界是等势面。
势函数定义：
正类样本：带正电荷
负类样本：带负电荷
空间任一点势能 = 所有样本在该点产生的势能之和
分类规则：
对于新样本 x：
势能（x）= ∑yi*K(x,xi)
K()是势能函数，类似核函数，如果势能>0表示正类，如果势能<0表示负类
常用势函数：高斯势函数，反距离势函数，多项式势函数
势函数是核方法的早期雏形，每个训练样本对空间势能有贡献，预测时聚合所用样本的影响，这是径向基函数网络和核SVM的思想源头

十一、聚类方法的选择
如果要处理一个涉及多个不同大小、形状和方向的聚类问题，可以选择DBSCAN和高斯混合模型GMM，不要选择Kmeans
Kmeans假设所有的聚类都是凸形的，且具有相同的大小和形状，因此对于这种情况它不是最佳选择。
DBSCAN是一种基于密度的聚类算法，它能够发现任意形状的聚类。
GMM是一个概率模型，它假设数据由多个高斯分布生成。通过调整高斯分布的参数，GMM可以适应不同大小、形状和方向的聚类，因此是一个好的选择。

十二、隐马 和 CRF
HMM和CRF都是自然语言处理中经典的时间序列问题，在时序问题上获得了广泛的应用。
HMM是生成模型，CRF是判别模型，生成模型需要由数据学习联合概率分布再求出条件概率分布进行预测 ，判别模型则直接学习决策函数或者条件概率分布。
CRF相比HMM，能利用更多特征信息，原因在于HMM为了计算上的方便，只使用了局部特征，比如转移概率只依赖于当前时刻或者前一个时刻，而且HMM中的概率
有一定的限制，比如要求概率在0和1之间且和为1等，而CRF能够利用更加丰富的特征信息。
CRF更适合在高度复杂和重叠的特征条件下使用，原因在于这种情况下数据的分布非常不清晰，HMM很难学习到联合概率分布。
（一）HMM
1、评估问题使用前向后向算法。该算法用于计算在给定模型参数和观测序列的情况下，出现观测序列的概率。前向算法从序列开始向后计算，后向算法从序列末尾
向前计算。说白话就是，前后向算法用于计算观察序列的概率。
2、解码问题使用维比特算法，该算法用于找出最可能产生观测序列的状态序列。维比特算法是一种动态规划算法，能高效找到最优路径。白话就是，用来寻找最可能的状态序列
3、学习问题使用Baum-Welch算法。该算法用于在已知观测序列的情况下，估计模型的参数（转移概率矩阵、发射概率矩阵）。它是一种期望最大化EM算法的特列。
EM算法在只知道观察序列，不知道状态序列的情况下进行参数估计，属于解决不完全数据的优化算法。

十三、几种距离
1、欧式距离
d(x,y)=Σ(xi-yi)^2
2、切比雪夫距离
d(x,y)=|x(imax)-y(imax)|,它是衡量的是两个点在各维度上的最大差值，没有考虑尺度和相关性的问题，会收到量纲影响。
3、马氏距离
d(x,y)=(x-y)^T ∑^(-1)(x-y)
x和y是两个样本点，这里∑表示协方差矩阵，用于描述数据集中各维度之间的相关性和尺度差异。∑^-1是协方差矩阵的逆矩阵。
4、汉明距离
用于计算离散变量，如字符串、二进制数
d(x,y)=不同位置的数量，
5、曼哈顿距离
d(x,y)=∑|xi-yi|
它是各个维度上绝对值差的总和。

十四、二分类问题
（一）基尼指数、分类误差率 和 二分之一熵
在二分类问题中，我们通常使用三种不纯度度量方法：基尼指数、分类误差率和熵。这三者在数值上有固定关系：分类误差率 < 基尼系数 < 二分之一熵
分类误差率=min(p, 1-p)
基尼指数=2p(1-p),【推导：基尼指数=1-∑pk^2,二分类的基尼指数=1-(p^2+(1-p)^2)】
二分之一熵=-1/2(plog2p-(1-p)log2(1-p))

十五、梯度下降中优化器的选择
（1）在处理海量数据，并且无法完全将数据加载到内存中时，内存效率和收敛稳定是考量的关键。
批量梯度下降需要整个数据集计算梯度，内存消耗大，不适合。随机梯度下降内存效率高，但是使用单个样本，噪声大，容易导致损失波动。
牛顿法需要计算Hessien矩阵，内存和计算成本高，不适合海量数据。
Adam优化器基于小批量实现，结合动量和自适应学习率，能够高效处理大规模数据。

十六、标准化和归一化
标准化是让均值为0方差为1，保证梯度下降稳定收敛。
归一化，缩放到0到1，对异常值敏感，优化梯度下降。

十七、EM算法
1、Kmeans算法的EM算法
E步骤（期望步骤），固定模型参数（簇中心），更新隐含参数（每个点的簇标号）
M步骤（最大化步骤）：固定隐含参数，更i性能模型参数以最大化目标函数。
数据点的簇标号作为在迭代过程中需要不断估计的变量，符号隐含参数的定义。也就是说，隐含参数指的是每个数据点的簇标号，因为这些簇标号在算法迭代过程中
不能直接观察到，需要迭代优化来估计。

十八、统计决策理论和假设检验
最大最小损失准则、最小误判概率准则、最小损失准则和NP判决
问题框架：
我们面临一个二分类决策问题：
H0:零假设（如信号不存在，正常状态）
H1：备择假设（如，信号存在，异常状态）
观察数x，需要接受H0还是H1

四个准则详解：
1、最小误判概率准测 Minimum Probability of Error
核心思想：最小化总的错误概率，不考虑不同错误的代价差异。
总错误概率：
Pe = P(判为H1|H0)P(H0) + P(判为H0|H1)P(H1)
决策规则：比较后验概率
如果P(H1|x)>P(H0|x)，判为H1
特点：假设两类错误代价相同，需要直到先验概率P(H0) 和 P(H1),在通信和模式识别中常用。

2、最小损失准测 （Minimum Expected Loss / Bayes Risk）
核心思想：考虑不同错误的不同代价，最小化期望损失（贝叶斯风险）
代价矩阵
                真实为H0               真实为H1
判为H0         C00（正确代价）          C01（漏报代价）
判为H1         C10（虚警报代价）         C11（正确代价）
一般C00=C11=0，C10>0,C01>0
期望损失（条件风险）
R(判为H0|x)=C00P(H0|x)+C01P(H1|x)
R(判为H1|x)=C10P(H0|x) + C11P(H1|x)
决策规则：
选择条件风险较小的假设
如果R(H1|x)<R(H0|x)，判为H1
同样需要先验概率

3、最大最小损失准则
核心思想：当先验概率未知或者不确定时，采取最坏情况下的最优策略
min maxR(P(H0),决策)
R是贝叶斯风险
关键性质：在最小损失准则下，存在一个特殊的先验P*(H0)，使得：
（1）该先验下的贝叶斯决策具有等价性质：
P(判为H1|H0)*（C10-C00）= P(判为H0|H1)*（C01-C11）
（2）这个决策规则就是最大最小决策规则
决策阈值就是P(x|H1)/P(x|H0)＞ [（C10-C00）/ （C01-C11）] * [P*(H0)/(1-P*(H0))]
其中，P*(H0)是使两类错误代价相等的先验。
应用场景：雷达检测（不知道信号出现的先验概率），安全系统设计（考虑最坏情况）

4、NP判决（Neyman-Pearson Criterion）
核心思想：在固定第一类错误概率（虚警概率下），最小化第二类错误概率（或者等价地，最大化检测概率）
数学形式：
约束化问题
        minP(判为H0|H1)  st. P(判为H1|H0)≤α，α是显著性水平(0.05)
决策规则：
似然比检验：
如果 L(x)=p(x|H1)/p(x|H0)，判为H1
P(L(x) > η|H0) = α
关键定理：Neyman-Pearson引理论：对于简单假设检验，似然比检验是最优的（在固定虚警概率下最大化检测概率）
应用场景：假设检验的经典框架、质量控制、科学实验。无需先验和代价。

十九、缺失值、归一化、离群点
树模型不需要归一化，直接用信息熵增益、信息增益比就可以处理。
树模型对确实值从早期的ID3就不敏感，有的是忽略，有的是填充
使用概率密度这些基本算法的对离群点、噪声点很敏感，比如高斯混合模型的聚类
